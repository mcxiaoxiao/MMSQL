{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "687eb5bd-ce5f-4e1d-94cf-4df4aab4063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                             | 0/148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________\n",
      "0\n",
      "answerable -- How many students in the \"Student\" table are over the age of 20?\n",
      "Next Turn predict_sql: SELECT COUNT(*) FROM Student WHERE Age > 20\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- How many of those students own pets?\n",
      "Next Turn predict_sql: SELECT COUNT(DISTINCT S.StuID) FROM Student AS S JOIN Has_Pet AS HP ON S.StuID = HP.StuID WHERE S.Age > 20\n",
      "Predict Type: answerable\n",
      "__________________\n",
      "0\n",
      "answerable -- Can you list all the continents?\n",
      "Next Turn predict_sql: SELECT Continent FROM continents\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- Okay, now can you list the number of countries belonging to each continent, including the continent id as well?\n",
      "Next Turn predict_sql: SELECT c.ContId, c.Continent, COUNT(DISTINCT co.CountryId) AS NumberOfCountries FROM continents c JOIN countries co ON c.ContId = co.Continent GROUP BY c.ContId, c.Continent ORDER BY c.ContId\n",
      "Predict Type: answerable\n",
      "2\n",
      "improper -- Thanks!\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                                                                                                                                 | 2/148 [00:05<06:36,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The user's message 'Thanks!' does not contain a question or request for information, it is simply a polite expression of gratitude. Therefore, the system's response 'You're welcome! Let me know if you have any other questions.' is appropriate and matches the social convention of responding to thanks. Since the user's input does not require a database query or any specific information related to the database, the criteria for relevance, clarity, completeness, accuracy, and utility do not apply in this context.\",\n",
      "  \"Score\": \"10\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "__________________\n",
      "0\n",
      "unanswerable -- How many customers are there now?\n",
      "Next Turn predict_sql: SELECT COUNT(*) FROM visitor\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- How many visitors are ther?\n",
      "Next Turn predict_sql: SELECT COUNT(DISTINCT ID) FROM visitor\n",
      "Predict Type: answerable\n",
      "2\n",
      "answerable -- How many of them have membership levels higher than 4?\n",
      "Next Turn predict_sql: SELECT count(*) FROM visitor WHERE Level_of_membership > 4\n",
      "Predict Type: answerable\n",
      "3\n",
      "answerable -- Can you please list their names?\n",
      "Next Turn predict_sql: SELECT Name FROM visitor WHERE Level_of_membership  >  4  \n",
      "Predict Type: answerable\n",
      "4\n",
      "answerable -- Can you rank them by their membership level from high to low?\n",
      "Next Turn predict_sql: SELECT name, Level_of_membership FROM visitor ORDER BY Level_of_membership DESC\n",
      "Predict Type: answerable\n",
      "5\n",
      "improper -- Thanks!\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                                                                                                                                                               | 3/148 [00:09<07:52,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The user's message 'Thanks!' typically indicates that the conversation is concluding and does not contain a question or request for further information. The system's response 'You're welcome! Let me know if you have any other questions.' is appropriate for a conversational closing and does not require any database-related information or action. Therefore, the criteria for relevance, clarity, completeness, accuracy, and utility do not apply as the user did not pose a question related to the database. The system's response is polite and offers assistance for any future inquiries, which is suitable for the context.\",\n",
      "  \"Score\": \"10\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "__________________\n",
      "0\n",
      "ambiguous -- Which cause has the least enrollment?\n",
      "Next Turn predict_sql: SELECT c.course_name FROM Courses c JOIN Student_Enrolment_Courses sec ON c.course_id = sec.course_id GROUP BY c.course_name ORDER BY COUNT(sec.student_course_id) ASC LIMIT 1\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- the name\n",
      "Next Turn predict_sql: SELECT T1.course_name FROM Courses AS T1 JOIN Student_Enrolment_Courses AS T2   ON T1.course_id = T2.course_id GROUP BY T1.course_name ORDER BY COUNT(T2.student_course_id) LIMIT 1\n",
      "Predict Type: answerable\n",
      "2\n",
      "answerable -- What is the course ID of Math?\n",
      "Next Turn predict_sql: SELECT course_id FROM Courses WHERE course_name = 'math'\n",
      "Predict Type: answerable\n",
      "3\n",
      "answerable -- How many students enrolled a course with the id of 14\n",
      "Next Turn predict_sql: SELECT COUNT(DISTINCT student_id) FROM Student_Enrolment_Courses WHERE course_id = 14\n",
      "Predict Type: answerable\n",
      "4\n",
      "improper -- Thanks!\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                                                                                                                              | 4/148 [00:13<08:47,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The user's message 'Thanks!' does not contain a question or request for information, it is simply an expression of gratitude. Therefore, the system's response 'You're welcome! Let me know if you have any other questions. ðŸ˜Š' is appropriate and polite, matching the social context of the conversation. Since there is no question asked, the criteria for relevance, clarity, completeness, accuracy, and utility do not apply as they are meant to evaluate responses to informational queries or tasks.\",\n",
      "  \"Score\": \"10\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "__________________\n",
      "0\n",
      "answerable -- Can you show the birth date of player id 200002?\n",
      "Next Turn predict_sql: SELECT birth_date FROM players WHERE player_id = 200002\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- What is the first name of player id 200001?\n",
      "Next Turn predict_sql: SELECT first_name FROM players WHERE player_id = 200001\n",
      "Predict Type: answerable\n",
      "2\n",
      "answerable -- Can you show the codes of countries that have more than 50 players?\n",
      "Next Turn predict_sql: SELECT country_code FROM players GROUP BY country_code HAVING COUNT(*) > 50 ```\n",
      "Predict Type: answerable\n",
      "3\n",
      "improper -- Thanks!\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                                                                                                                            | 5/148 [00:17<08:54,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The user's message 'Thanks!' does not contain a question or request for information, it is simply a polite expression of gratitude. The system's response 'You're welcome! Let me know if you have any other questions. ðŸ˜Š' is a courteous acknowledgment and an invitation for further interaction. Since there is no question asked, the criteria for relevance, clarity, completeness, accuracy, and utility do not apply as they are meant to evaluate responses to questions or requests for information.\",\n",
      "  \"Score\": \"10\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "__________________\n",
      "0\n",
      "answerable -- What is the average age of the losers from all matches?\n",
      "request gpt-4-1106-preview\n",
      "LLM Response: {\n",
      "  \"AnswerType\": \"unanswerable\",\n",
      "  \"Rationale\": \"The system did not provide any response to the user's question. The user asked for the average age of the losers from all matches, which would require calculating the average based on the 'loser_age' field in the 'matches' table. However, the system did not output any information, calculation, or SQL query that would lead to the answer. Therefore, the response cannot be evaluated based on the criteria of relevance, clarity, completeness, accuracy, and utility, as there is no response to assess.\",\n",
      "  \"Score\": \"0\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: unanswerable\n",
      "1\n",
      "answerable -- What about the average age of the winners from all matches?\n",
      "Next Turn predict_sql: SELECT avg(winner_age) FROM matches \n",
      "Predict Type: answerable\n",
      "2\n",
      "answerable -- How many winners are left handed?\n",
      "Next Turn predict_sql: SELECT count(DISTINCT winner_id) FROM matches WHERE winner_hand = 'L'  \n",
      "Predict Type: answerable\n",
      "3\n",
      "answerable -- What are the names of the winner and loser from the longest match?\n",
      "Next Turn predict_sql: SELECT winner_name, loser_name FROM matches ORDER BY minutes DESC LIMIT 1  \n",
      "Predict Type: answerable\n",
      "4\n",
      "improper -- Thanks!\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                                                                                                                          | 6/148 [00:25<12:24,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The user's message 'Thanks!' does not contain a question or request for information. It is a polite expression of gratitude, typically signaling the end of a conversation. The system's response 'You're welcome! Let me know if you have any other questions. ðŸ˜Š' is appropriate and polite, acknowledging the user's thanks and offering further assistance if needed. Since there is no question asked, the criteria for relevance, clarity, completeness, accuracy, and utility do not apply as they are meant to evaluate responses to questions or requests for information.\",\n",
      "  \"Score\": \"10\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "__________________\n",
      "0\n",
      "answerable -- which models are lighter than 3500?\n",
      "Next Turn predict_sql: SELECT DISTINCT   Model FROM car_names AS cn JOIN cars_data AS cd   ON cn.MakeId = cd.Id WHERE   cd.Weight < 3500\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- what is the weight of  BMW?\n",
      "Next Turn predict_sql: SELECT T3.Weight FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON T1.Id  =  T2.Maker JOIN CARS_DATA AS T3 ON T2.ModelId  =  T3.Id WHERE T1.Maker  =  'bmw' \n",
      "Predict Type: answerable\n",
      "2\n",
      "ambiguous -- what is the model name of the id 2?\n",
      "Next Turn predict_sql: SELECT Model FROM car_names WHERE MakeId = 2 \n",
      "Predict Type: answerable\n",
      "3\n",
      "answerable -- carmaker?\n",
      "Next Turn predict_sql: SELECT FullName FROM car_makers WHERE Id = 2\n",
      "Predict Type: answerable\n",
      "4\n",
      "improper -- Thanks!\n",
      "Next Turn does not exist.\n",
      "__________________\n",
      "0\n",
      "answerable -- Which model saves the most gasoline? That is to say, have the maximum miles per gallon?\n",
      "Next Turn predict_sql: SELECT Model FROM model_list ml JOIN car_names cn ON ml.ModelId = cn.MakeId JOIN cars_data cd ON cn.MakeId = cd.Id ORDER BY cd.MPG DESC LIMIT 1\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- Which model has the highest horsepower?\n",
      "Next Turn predict_sql: SELECT T1.Model FROM CAR_NAMES AS T1 JOIN CARS_DATA AS T2 ON T1.MakeId = T2.Id ORDER BY T2.Horsepower DESC LIMIT 1\n",
      "Predict Type: answerable\n",
      "__________________\n",
      "0\n",
      "answerable -- How many documents have template type code BK?\n",
      "Next Turn predict_sql: SELECT COUNT(DISTINCT D.Document_ID) FROM Documents AS D JOIN Templates AS T ON D.Template_ID = T.Template_ID JOIN Ref_Template_Types AS RTT ON T.Template_Type_Code = RTT.Template_Type_Code WHERE RTT.Template_Type_Code = 'BK'\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- And their names?\n",
      "Next Turn predict_sql: SELECT T2.Document_Name FROM Templates AS T1 JOIN Documents AS T2 ON T1.Template_ID = T2.Template_ID WHERE T1.Template_Type_Code = 'BK'\n",
      "Predict Type: answerable\n",
      "2\n",
      "answerable -- How many documents do not have template type code BK?\n",
      "Next Turn predict_sql: SELECT count(*) FROM Documents AS T1 JOIN Templates AS T2 ON T1.template_id  =  T2.template_id WHERE T2.template_type_code != \"BK\" \n",
      "Predict Type: answerable\n",
      "3\n",
      "improper -- Great! I appreciate your help. Thanks!\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                                                                                                                     | 9/148 [00:29<06:48,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The user's message 'Great! I appreciate your help. Thanks!' does not contain a question or request for further information. It is a statement of gratitude and closure to the conversation. The system's response is a polite acknowledgment and an offer for further assistance, which is appropriate in a conversational context. Therefore, the criteria for relevance, clarity, completeness, accuracy, and utility do not apply as there is no question being asked that requires a factual or informative response.\",\n",
      "  \"Score\": \"10\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "__________________\n",
      "0\n",
      "answerable -- Show the name and the release year of the song by the youngest singer\n",
      "Next Turn predict_sql: SELECT Name, Song_release_year FROM singer ORDER BY Age LIMIT 1\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- Ok. Can you show me the name and the release year of the song by the oldest singer\n",
      "Next Turn predict_sql: SELECT song_name, song_release_year FROM singer ORDER BY age DESC LIMIT 1\n",
      "Predict Type: answerable\n",
      "2\n",
      "answerable -- Can you also show me the name of the youngest singer\n",
      "Next Turn predict_sql: SELECT Name FROM singer ORDER BY Age LIMIT 1\n",
      "Predict Type: answerable\n",
      "3\n",
      "answerable -- And what is the name of the oldest singer and his age\n",
      "Next Turn predict_sql: SELECT Name, Age FROM singer ORDER BY Age DESC LIMIT 1  \n",
      "Predict Type: answerable\n",
      "4\n",
      "answerable -- and what country was he from?\n",
      "\n",
      "Next Turn predict_sql: SELECT Country FROM singer ORDER BY age DESC LIMIT 1  \n",
      "Predict Type: answerable\n",
      "5\n",
      "answerable -- give me the names of the singers from United States\n",
      "\n",
      "Next Turn predict_sql: SELECT Name FROM singer WHERE Country = 'United States'  \n",
      "Predict Type: answerable\n",
      "6\n",
      "improper -- Thanks!\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                                                                                                                  | 10/148 [00:34<07:41,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The user's message 'Thanks!' does not contain a question or request for information, it is a polite expression of gratitude. The system's response 'You're welcome! Let me know if you have any other questions. ðŸ˜Š' is a courteous acknowledgment of the user's thanks and an offer to assist further, which is appropriate in a conversational context. The reference answer 'goodbye!' is also a valid conversational closure but does not directly address the user's expression of thanks. Since the user's message does not require a database-related response or any specific action, the criteria for relevance, clarity, completeness, accuracy, and utility do not apply in this context.\",\n",
      "  \"Score\": \"10\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "__________________\n",
      "0\n",
      "answerable -- Which country tv channels some cartoon written by todd casey?\n",
      "Next Turn predict_sql: SELECT DISTINCT T1.Country FROM TV_Channel AS T1 JOIN Cartoon AS T2 ON T1.id = T2.Channel WHERE T2.Written_by = 'Todd Casey'\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- What is the title of cartoon written by todd casey?\n",
      "Next Turn predict_sql: SELECT Title FROM Cartoon WHERE Written_by = 'Todd Casey'\n",
      "Predict Type: answerable\n",
      "2\n",
      "answerable -- Who directed the cartoon titled enter the outsiders?\n",
      "Next Turn predict_sql: SELECT Directed_by FROM Cartoon WHERE Title = 'Enter the Outsiders' ```\n",
      "Predict Type: answerable\n",
      "3\n",
      "improper -- Thanks!\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                                                                                                                                                | 11/148 [00:40<09:00,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The user's message 'Thanks!' typically indicates that the conversation is concluding or that the user is expressing gratitude, rather than asking a question or requesting further information. The system's response 'You're welcome! Let me know if you have any other questions. ðŸ˜Š' is appropriate for a conversational AI that is not limited to SQL-based interactions. Since the user did not provide a query or context that required an SQL-based response or further clarification, the system's response is proper in a general conversational context but improper for an SQL-based query system. Therefore, the criteria for relevance, clarity, completeness, accuracy, and utility do not apply as the user did not pose a question that required an SQL-based answer or any detailed response.\",\n",
      "  \"Score\": \"10\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "__________________\n",
      "0\n",
      "ambiguous -- What is the weight of id 1?\n",
      "Next Turn predict_sql: SELECT Weight FROM cars_data WHERE Id = 1\n",
      "Predict Type: answerable\n",
      "1\n",
      "improper -- Yes\n",
      "Next Turn predict_sql: SELECT Weight FROM cars_data WHERE Id = 1\n",
      "Predict Type: answerable\n",
      "2\n",
      "ambiguous -- Which year launch the car model  buick?\n",
      "Next Turn predict_sql: SELECT DISTINCT T3.Year FROM car_names AS T1 JOIN model_list AS T2 ON T1.MakeId = T2.ModelId JOIN cars_data AS T3 ON T3.Id = T2.ModelId WHERE T1.Make = 'buick' ```\n",
      "Predict Type: answerable\n",
      "3\n",
      "improper -- Yes\n",
      "Next Turn predict_sql: SELECT DISTINCT T3.Year FROM car_names AS T1 JOIN model_list AS T2 ON T1.MakeId = T2.Maker JOIN cars_data AS T3 ON T3.Id = T2.ModelId WHERE T1.Model = 'buick' ```\n",
      "Predict Type: answerable\n",
      "4\n",
      "answerable -- Find the horsepower of model plymouth.\n",
      "Next Turn predict_sql: SELECT T2.Horsepower FROM CAR_NAMES AS T1 JOIN CARS_DATA AS T2 ON T1.MakeId  =  T2.Id WHERE T1.Model = \"plymouth\"  \n",
      "Predict Type: answerable\n",
      "5\n",
      "answerable -- For the cars with 4 cylinders, which model has the largest horspower?\n",
      "Next Turn predict_sql: SELECT T1.Model FROM CAR_NAMES AS T1 JOIN CARS_DATA AS T2 ON T1.MakeId = T2.Id WHERE T2.Cylinders = 4 ORDER BY T2.Horsepower DESC LIMIT 1 ```\n",
      "Predict Type: answerable\n",
      "6\n",
      "improper -- Thanks!\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                                                                                                               | 12/148 [00:44<08:51,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The user's message 'Thanks!' does not contain a question or request for information, it is a polite expression of gratitude. Therefore, the system's response is appropriate as it acknowledges the user's thanks and offers further assistance. The criteria of relevance, clarity, completeness, accuracy, and utility do not apply in this context because there is no question being asked that requires a detailed or informative response.\",\n",
      "  \"Score\": \"10\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "__________________\n",
      "0\n",
      "answerable -- What are the codes of the countries that do not speak English?\n",
      "Next Turn predict_sql: SELECT DISTINCT CountryCode FROM countrylanguage WHERE Language != 'English' ```\n",
      "Predict Type: answerable\n",
      "1\n",
      "answerable -- What are the codes of the countries that do not speak English whose government forms are not Republic?\n",
      "request gpt-4-1106-preview\n",
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The system did not provide any response to the user's question. The user asked for the codes of countries that do not speak English and whose government forms are not Republic. This requires a query to the database to filter out countries based on the specified language and government form criteria. Since there is no system response provided, we cannot evaluate the relevance, clarity, completeness, accuracy, or utility of the response.\",\n",
      "  \"Score\": \"0\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "2\n",
      "answerable -- What language do each of those speak, please?\n",
      "Next Turn predict_sql: SELECT T1.Code, T2.Language FROM country AS T1 INNER JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T1.GovernmentForm != 'Republic' EXCEPT SELECT T2.CountryCode, T2.Language FROM countrylanguage AS T2 WHERE T2.Language = 'English' ```\n",
      "Predict Type: answerable\n",
      "3\n",
      "answerable -- Which of those countries is on the continent North America?\n",
      "Next Turn predict_sql: SELECT T1.Code FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T1.Continent = 'North America' AND T2.Language != 'English' AND T1.GovernmentForm != 'Republic'  \n",
      "Predict Type: answerable\n",
      "4\n",
      "improper -- Thanks!\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                                                                                                             | 13/148 [00:53<11:40,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"improper\",\n",
      "  \"Rationale\": \"The user's message 'Thanks!' does not contain a question or request for information, it is a polite closing to a conversation. The system's response 'You're welcome! Let me know if you have any other questions. ðŸ˜Š' is appropriate and polite, matching the social context of the user's message. Since there is no question asked, the criteria for relevance, clarity, completeness, accuracy, and utility do not apply in this context. The system's response is a standard closing interaction and does not require any action or information related to the database schema provided.\",\n",
      "  \"Score\": \"10\"\n",
      "}\n",
      "Next Turn predict_sql: \n",
      "Predict Type: improper\n",
      "__________________\n",
      "0\n",
      "unanswerable -- What is the most popular car color?\n",
      "request gpt-4-1106-preview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                                                                                                                             | 13/148 [01:00<10:23,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: {\n",
      "  \"AnswerType\": \"unanswerable\",\n",
      "  \"Rationale\": {\n",
      "    \"Relevance\": \"The response is fully relevant as it correctly identifies that the database does not contain information about car colors, which is what the user's question pertains to.\",\n",
      "    \"Clarity\": \"The response is very clear and easy to understand. It directly states that the database does not contain the requested information.\",\n",
      "    \"Completeness\": \"The response is complete in the context of the information available in the database. It cannot provide information that does not exist within the given schema.\",\n",
      "    \"Accuracy\": \"The response is completely accurate as it truthfully reflects the absence of data regarding car colors in the database.\",\n",
      "    \"Utility\": \"The response meets the user's needs to the extent possible by explaining that the question cannot be answered with the current database schema.\"\n",
      "  },\n",
      "  \"Score\": {\n",
      "    \"Relevance\": 2,\n",
      "    \"Clarity\": 2,\n",
      "    \"Completeness\": 2,\n",
      "    \"Accuracy\": 2,\n",
      "    \"Utility\": 2\n",
      "  },\n",
      "  \"TotalScore\": 10\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 183\u001b[0m\n\u001b[0;32m    181\u001b[0m input_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/gemini-1-Copy1.5-flash-llm.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    182\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/rqs_gemini-1-Copy1.5-flash-llm.json.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 183\u001b[0m process_turns(input_file_path, output_file_path)\n",
      "Cell \u001b[1;32mIn[8], line 167\u001b[0m, in \u001b[0;36mprocess_turns\u001b[1;34m(file_path, output_path)\u001b[0m\n\u001b[0;32m    165\u001b[0m next_turn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot answerable\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Ask LLM, Get categorized and RQS scored based on database, questions, answers, gold answer\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m type_ai, rqs_ai, rationale_ai \u001b[38;5;241m=\u001b[39m ask_ai(db_name,turns[i]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),predict_text,next_turn\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)) \n\u001b[0;32m    168\u001b[0m next_turn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m type_ai\n\u001b[0;32m    169\u001b[0m next_turn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRQS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m rqs_ai\n",
      "Cell \u001b[1;32mIn[8], line 108\u001b[0m, in \u001b[0;36mask_ai\u001b[1;34m(db_name, question, answer_pred, answer_gold)\u001b[0m\n\u001b[0;32m    105\u001b[0m rationale_ai \u001b[38;5;241m=\u001b[39m response_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRationale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# æ£€æŸ¥è¿”å›žçš„ç±»åž‹å’Œåˆ†æ•°æ˜¯å¦ç¬¦åˆé¢„æœŸ\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_ai \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimproper\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munanswerable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mambiguous\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mint\u001b[39m(rqs_ai) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mint\u001b[39m(rqs_ai) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m type_ai, rqs_ai, rationale_ai\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'dict'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RQS_eval.py\n",
    "\n",
    "Reads a specified JSON file and iterates through each object's 'turns' array to find a turn where 'isuser' is true and its following turn.\n",
    "\n",
    "Usage:\n",
    "    python RQS_eval.py outputs/llm_responses.json outputs/llm_responses_rqs.json\n",
    "\n",
    "Arguments:\n",
    "    --file_path: Path to the JSON file.\n",
    "    --output_path: Path where the modified JSON file will be saved.\n",
    "\"\"\"\n",
    "\n",
    "from tools.api_request import request_gemini as request_llm\n",
    "from tools.db_detail import db_getdesc\n",
    "from tools.sql_execute import sqlite_execute as execute\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sql_evoke(query,db_name):\n",
    "    result, execution_time ,executable = execute(\"datasets/cosql_dataset/database/\"+db_name+\"/\"+db_name+\".sqlite\",query)\n",
    "    return result \n",
    "\n",
    "def get_example(db_name):\n",
    "    sql_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    result = sql_evoke(sql_query,db_name)\n",
    "    column_example=\"\"\n",
    "    for table_name in result:\n",
    "        column_example = column_example + table_name[0] + \":\\n\"\n",
    "        sql_get_eg = \"SELECT * FROM \"+ table_name[0] +\" LIMIT 3;\"\n",
    "        table_eg = sql_evoke(sql_get_eg,db_name)\n",
    "        for table_data in table_eg:\n",
    "            column_example = column_example + '('\n",
    "            for column_data in table_data: \n",
    "                column_example = column_example + str(column_data) +','\n",
    "            column_example = column_example[:-1] + ')\\n'\n",
    "    return column_example\n",
    "    \n",
    "\n",
    "def ask_ai(db_name, question, answer_pred, answer_gold):\n",
    "    description = db_getdesc(db_name)\n",
    "    column_example = get_example(db_name)\n",
    "    template = \"\"\"\n",
    "{database_description}\n",
    "\n",
    "{user_question}\n",
    "\n",
    "{system_response}\n",
    "\n",
    "{reference_answer}\n",
    "\n",
    "Evaluate the quality of the system's response based on the following criteria. Assign 2 points directly if a criterion does not apply.\n",
    "Relevance:\n",
    "0 points: The response is completely irrelevant.\n",
    "1 point: The response is partially relevant but misses key details.\n",
    "2 points: The response is fully relevant and addresses the question adequately.\n",
    "Clarity:\n",
    "0 points: The response is incomprehensible.\n",
    "1 point: The response is mostly clear with minor ambiguities.\n",
    "2 points: The response is very clear and easy to understand.\n",
    "Completeness:\n",
    "0 points: The response does not address the question at all.\n",
    "1 point: The response covers most aspects of the question but lacks some details.\n",
    "2 points: The response thoroughly addresses all aspects of the question.\n",
    "Accuracy:\n",
    "0 points: The response contains factually incorrect information.\n",
    "1 point: The response is partially accurate with some errors.\n",
    "2 points: The response is completely accurate.\n",
    "Utility:\n",
    "0 points: The response does not meet the user's needs or explain the context of the question.\n",
    "1 point: The response somewhat meets the user's needs and provides partial explanations.\n",
    "2 points: The response excellently meets the user's needs and clearly explains the context or ambiguity of the question.\n",
    "Task:\n",
    "Classify the Response: Determine if the system response is 'improper'(Non-SQL based user questions), 'unanswerable'(unachievable under existing conditions), or 'ambiguous'(Lack of clarity).\n",
    "Evaluate Each Criterion: Provide a detailed rationale for the score assigned to each criterion.\n",
    "Calculate the Total Score: Sum the scores for all criteria.\n",
    "\n",
    "Output Format:\n",
    "{{\n",
    "  \"AnswerType\": \"\",\n",
    "  \"Rationale\": \"\",\n",
    "  \"Score\": \"\"\n",
    "}}\n",
    "    \"\"\"\n",
    "    filled_template = template.format(\n",
    "        database_description=\"Database Description:\"+ \"\\nDatabase schema:\\n\" + description + \"\\nExamples for each table:\"+ column_example,\n",
    "        user_question=\"User Question:\" + question,\n",
    "        system_response=\"System Response:\" + answer_pred,\n",
    "        reference_answer=\"Reference Answer:\" + answer_gold\n",
    "    )\n",
    "    \n",
    "    # print(filled_template)\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": filled_template}]\n",
    "    max_attempts = 10\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        llm_response = request_llm(messages)\n",
    "        print(\"LLM Response:\", llm_response)\n",
    "        \n",
    "        try:\n",
    "            response_data = json.loads(llm_response)\n",
    "            type_ai = response_data.get(\"AnswerType\", \"\")\n",
    "            rqs_ai = response_data.get(\"Score\", 0)\n",
    "            rationale_ai = response_data.get(\"Rationale\", \"\")\n",
    "            \n",
    "            # æ£€æŸ¥è¿”å›žçš„ç±»åž‹å’Œåˆ†æ•°æ˜¯å¦ç¬¦åˆé¢„æœŸ\n",
    "            if type_ai in [\"improper\", \"unanswerable\", \"ambiguous\"] and int(rqs_ai) >= 0 and int(rqs_ai) <= 10:\n",
    "                return type_ai, rqs_ai, rationale_ai\n",
    "            else:\n",
    "                raise ValueError(\"Response type or score out of expected range.\")\n",
    "        except (json.JSONDecodeError, KeyError, ValueError, TypeError, Exception) as e:\n",
    "            print(\"\\033[91mRetry Reason: {}\\033[0m\".format(str(e)))  # çº¢è‰²å­—ä½“æç¤ºé‡è¯•åŽŸå› \n",
    "            attempt += 1\n",
    "    return \"\\033[91mFailed to obtain valid response\\033[0m\", 0, \"No valid response after {} attempts\".format(max_attempts)\n",
    "\n",
    "def process_turns(file_path, output_path):\n",
    "\n",
    "    # Open and read the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Iterate over each object in the file\n",
    "    for entry in tqdm(data):\n",
    "        # Check if the 'turns' key exists\n",
    "        print(\"__________________\")\n",
    "        if 'turns' in entry:\n",
    "            turns = entry['turns']\n",
    "            db_name = entry['db_name']\n",
    "            length = len(turns)\n",
    "            # Iterate over each turn in 'turns'\n",
    "            for i in range(length):\n",
    "                # Check if the current turn is a user turn\n",
    "                if turns[i].get('isuser', False):\n",
    "                    print(i//2)\n",
    "                    # Output the current user's turn\n",
    "                    print(turns[i].get('type', ''), \"--\", turns[i].get('text', ''))\n",
    "                    # Check and process the next turn (if it exists)\n",
    "                    if i + 1 < length:\n",
    "                        next_turn = turns[i + 1]\n",
    "                        predict_text = next_turn.get('predict', '')\n",
    "                        # Find the positions of SELECT and the semicolon\n",
    "                        select_pos = predict_text.upper().find('SELECT')\n",
    "                        colon_pos = predict_text.find(';', select_pos)\n",
    "                        if select_pos != -1 and colon_pos != -1:\n",
    "                            predict_sql = predict_text[select_pos:colon_pos].replace('\\n',' ')\n",
    "                        elif select_pos != -1:\n",
    "                            predict_sql = predict_text[select_pos:].replace('\\n',' ')\n",
    "                        else:\n",
    "                            predict_sql = \"\"\n",
    "                        # Store the result in a new field 'predict_sql'\n",
    "                        next_turn['predict_sql'] = predict_sql\n",
    "                        # Calculate the ratio of the extracted SQL to the entire predict field\n",
    "                        if len(predict_text) == 0:\n",
    "                            ratio = 0\n",
    "                        else:\n",
    "                            ratio = len(predict_sql) / len(predict_text)\n",
    "                        if predict_sql != \"\" and ratio >= 0.5:\n",
    "                                next_turn['predict_type'] = 'answerable'\n",
    "                                if turns[i].get('type', '') == 'answerable':\n",
    "                                    next_turn['RQS'] = \"N/A\"\n",
    "                                else:\n",
    "                                    next_turn['RQS'] = 0\n",
    "                        else:\n",
    "                            next_turn['predict_type'] = 'not answerable'\n",
    "                            # Ask LLM, Get categorized and RQS scored based on database, questions, answers, gold answer\n",
    "                            type_ai, rqs_ai, rationale_ai = ask_ai(db_name,turns[i].get('text', ''),predict_text,next_turn.get('text', '')) \n",
    "                            next_turn['predict_type'] = type_ai\n",
    "                            next_turn['RQS'] = rqs_ai\n",
    "                            next_turn['RQS_Rationale'] = rationale_ai\n",
    "                        print(\"Next Turn predict_sql:\", predict_sql)\n",
    "                        print(\"Predict Type:\", next_turn['predict_type'])\n",
    "                    else:\n",
    "                        print(\"Next Turn does not exist.\")\n",
    "    \n",
    "    # Save the modified data to a new JSON file\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "# Example usage\n",
    "input_file_path = 'outputs/gemini-1-Copy1.5-flash-llm.json'\n",
    "output_file_path = 'outputs/rqs_gemini-1-Copy1.5-flash-llm.json.json'\n",
    "process_turns(input_file_path, output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
